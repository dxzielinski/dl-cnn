{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Using device: NVIDIA GeForce RTX 4060 Ti\n",
      "VRAM: 8 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from model_utils import Model, ClassificationData\n",
    "\n",
    "print(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0))\n",
    "print(\"VRAM:\", round(torch.cuda.get_device_properties(0).total_memory / 1024 ** 3), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "hyperparams = {\n",
    "   \"learning_rate\": [1e-1],\n",
    "   \"batch_size\": [512],\n",
    "    \"dropout\": [0.2, 0.3, 0.4],\n",
    "   \"weight_decay\": [1e-3],\n",
    "}\n",
    "runs = 10\n",
    "example_hyperparams = {\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 128,\n",
    "    \"dropout\": 0.4,\n",
    "    \"weight_decay\": 1e-3,\n",
    "}\n",
    "# iterate over all hyperparameters\n",
    "for lr in hyperparams[\"learning_rate\"]:\n",
    "    for bs in hyperparams[\"batch_size\"]:\n",
    "        for wd in hyperparams[\"weight_decay\"]:\n",
    "            for dp in hyperparams[\"dropout\"]:\n",
    "                example_hyperparams[\"learning_rate\"] = lr\n",
    "                example_hyperparams[\"batch_size\"] = bs\n",
    "                example_hyperparams[\"weight_decay\"] = wd\n",
    "                example_hyperparams[\"dropout\"] = dp\n",
    "                for i in range(runs):\n",
    "                    seed = 123 + i\n",
    "                    L.seed_everything(seed)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    model = Model(example_hyperparams)\n",
    "                    logger = MLFlowLogger(save_dir=\"mlruns\", experiment_name=\"CNN\")\n",
    "                    early_stop = EarlyStopping(monitor=\"train_loss\", patience=3, mode=\"min\", verbose=True, min_delta=0.01)\n",
    "                    checkpoint_callback = ModelCheckpoint(monitor=\"val_f1_macro\", mode=\"max\", dirpath=\"checkpoints\", filename=f\"run_{i}_lr={model.hyperparameters[\"learning_rate\"]}_bs={model.hyperparameters[\"batch_size\"]}_wd={model.hyperparameters[\"weight_decay\"]}_dropout={model.hyperparameters[\"dropout\"]}_seed={seed}\" + \"-{epoch:02d}-{val_f1_macro:.2f}\")\n",
    "                    trainer = L.Trainer(max_epochs=20, logger=logger, num_sanity_val_steps=0, enable_model_summary=False, deterministic=False, callbacks=[early_stop, checkpoint_callback], precision=\"16-mixed\")\n",
    "                    data = ClassificationData(batch_size=model.hyperparameters[\"batch_size\"])\n",
    "                    trainer.fit(model, datamodule=data)\n",
    "                    trainer.test(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
